{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1862ba9a-2d86-4b5e-b9c8-8b0022511963",
   "metadata": {},
   "source": [
    "<center><h1>Real Time Monitoring of Water Distribution System</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84069783-a384-43b2-bfda-9164287c9481",
   "metadata": {},
   "source": [
    "\n",
    "# Installing Required Libraries\n",
    "\n",
    "This cell installs necessary libraries like `kafka-python`, which allows Python to interact with Kafka. It will attempt to install the library if it is not already present in the environment. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22850b17-4332-4e6f-9c3f-486a8288130e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kafka-python in ./.local/lib/python3.10/site-packages (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48b46dc1-8188-40a5-9ec8-0c3ff202e5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.local/lib/python3.10/site-packages (from pandas) (2.1.2)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64339910-c446-4eec-b04b-fb42b847cb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tkinter (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tkinter\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tkinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37d38de7-12cb-4191-991f-4335977d6955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy!=1.24.0,>=1.20 in ./.local/lib/python3.10/site-packages (from seaborn) (2.1.2)\n",
      "Requirement already satisfied: pandas>=1.2 in ./.local/lib/python3.10/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in ./.local/lib/python3.10/site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.4.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/lib/python3/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.2->seaborn) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a7fbd61-66ad-4be7-b087-2a00a33c88ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting dash\n",
      "  Downloading dash-2.18.1-py3-none-any.whl (7.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from dash) (2.32.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from dash) (4.6.4)\n",
      "Collecting retrying\n",
      "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
      "Collecting Werkzeug<3.1\n",
      "  Downloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting dash-core-components==2.0.0\n",
      "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
      "Requirement already satisfied: plotly>=5.0.0 in ./.local/lib/python3.10/site-packages (from dash) (5.24.1)\n",
      "Collecting dash-html-components==2.0.0\n",
      "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from dash) (59.6.0)\n",
      "Requirement already satisfied: nest-asyncio in ./.local/lib/python3.10/site-packages (from dash) (1.6.0)\n",
      "Collecting dash-table==5.0.0\n",
      "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in ./.local/lib/python3.10/site-packages (from dash) (4.12.2)\n",
      "Collecting Flask<3.1,>=1.0.4\n",
      "  Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting blinker>=1.6.2\n",
      "  Downloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Collecting itsdangerous>=2.1.2\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in ./.local/lib/python3.10/site-packages (from Flask<3.1,>=1.0.4->dash) (3.1.4)\n",
      "Collecting click>=8.1.3\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in ./.local/lib/python3.10/site-packages (from plotly>=5.0.0->dash) (24.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in ./.local/lib/python3.10/site-packages (from plotly>=5.0.0->dash) (9.0.0)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->dash) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->dash) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->dash) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests->dash) (3.4.0)\n",
      "Requirement already satisfied: six>=1.7.0 in /usr/lib/python3/dist-packages (from retrying->dash) (1.16.0)\n",
      "Installing collected packages: dash-table, dash-html-components, dash-core-components, retrying, MarkupSafe, itsdangerous, click, blinker, Werkzeug, Flask, dash\n",
      "Successfully installed Flask-3.0.3 MarkupSafe-3.0.2 Werkzeug-3.0.6 blinker-1.8.2 click-8.1.7 dash-2.18.1 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 itsdangerous-2.2.0 retrying-1.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4545f7e8-680b-49d2-a500-dcc0fb4b344c",
   "metadata": {},
   "source": [
    "# Generating Synthetic Data\n",
    "\n",
    "In this section, we generate synthetic data to simulate real-world events. This data will be sent to a Kafka topic, allowing us to test the Kafka producer and consumer functionality. By generating data with random values, timestamps, or other features, we can simulate scenarios like sensor readings, financial transactions, or user activities.\n",
    "\n",
    "Using Python’s `random` and `datetime` libraries, we generate numerical data with timestamps. This example creates random integer and float values to represent data like sensor readings . Each entry will have:\n",
    "## Unique timestamp\n",
    "> Denoting the time at which the Sensor reading were taken\n",
    "## Flow rate  \n",
    "> I have chosen Normal Distribution because flow rate typically fluctuates around an average value, with occasional spikes or dips. The normal distribution captures this behavior with a bell-shaped curve where most values fall near the mean and fewer occur at the extremes.\n",
    "## Pressure\n",
    "> Pressure in a water distribution system is unlikely to be negative, but it can vary significantly. The log-normal distribution is skewed towards positive values and has a long tail, making it suitable for modeling pressure readings where most values are positive and some can be much higher.\n",
    "## Temperature\n",
    "> Similar to flow rate, temperature in a water distribution system might fluctuate around an average value with occasional deviations\n",
    "## pH\n",
    "> Ideally, the pH of water should be close to neutral (pH = 7). The normal distribution with a small standard deviation (sigma) allows for slight variations around this ideal value while keeping most readings within a narrow acceptable range.\n",
    "## Turbidity\n",
    ">Turbidity refers to the cloudiness of water. Similar to pressure, turbidity readings are unlikely to be negative and can vary considerably. The log-normal distribution with a low mean reflects this, with most values being low (clear water) and some potential for higher turbidity levels.\n",
    "\n",
    "This data structure will be published to Kafka as individual JSON messages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12fd17d5-d8a7-46ab-84b6-769aa05ae05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to generate random water monitoring data for a single sensor\n",
    "def generate_sensor_data(sensor_id, start_time, num_records):\n",
    "    data = []\n",
    "    for i in range(num_records):\n",
    "        timestamp = start_time + timedelta(minutes=i * 10)  # Data recorded every 10 minutes\n",
    "        \n",
    "        # More realistic distributions\n",
    "        flow_rate = round(random.gauss(mu=50, sigma=10), 2)  # Normal distribution for flow rate\n",
    "        pressure = round(np.random.lognormal(mean=4, sigma=0.25), 2)  # Log-normal for pressure\n",
    "        temperature = round(random.gauss(mu=20, sigma=5), 2)  # Normal distribution for temperature\n",
    "        pH = round(random.gauss(mu=7, sigma=0.5), 2)  # Normal distribution for pH\n",
    "        turbidity = round(np.random.lognormal(mean=0.5, sigma=0.75), 2)  # Log-normal for turbidity\n",
    "\n",
    "        data.append([sensor_id, timestamp, flow_rate, pressure, temperature, pH, turbidity])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Function to generate data for multiple sensors\n",
    "def generate_water_distribution_data(num_sensors, num_records_per_sensor, start_time):\n",
    "    all_data = []\n",
    "    for sensor_id in range(1, num_sensors + 1):\n",
    "        sensor_data = generate_sensor_data(f\"Sensor_{sensor_id}\", start_time, num_records_per_sensor)\n",
    "        all_data.extend(sensor_data)\n",
    "    \n",
    "    columns = ['Sensor ID', 'Timestamp', 'Flow_Rate', 'Pressure', 'Temperature', 'pH', 'Turbidity']\n",
    "    df = pd.DataFrame(all_data, columns=columns)\n",
    "    return df\n",
    "\n",
    "# Parameters for the data generation\n",
    "num_sensors = 3  # Number of sensors in the system\n",
    "num_records_per_sensor = 14400  # Number of records per sensor (e.g., 144 records for 1 day at 10-minute intervals)\n",
    "start_time = datetime.now() - timedelta(days=100)  # Start time set to 24 hours ago\n",
    "\n",
    "# Generate the synthetic data\n",
    "df = generate_water_distribution_data(num_sensors, num_records_per_sensor, start_time)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv('water_distribution_monitoring_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5155f11-2212-4ef8-8dad-0b5395c039c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sensor ID</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Flow_Rate</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>pH</th>\n",
       "      <th>Turbidity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sensor_1</td>\n",
       "      <td>2024-08-03 19:13:04.347273</td>\n",
       "      <td>38.99</td>\n",
       "      <td>52.90</td>\n",
       "      <td>21.92</td>\n",
       "      <td>7.73</td>\n",
       "      <td>2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sensor_1</td>\n",
       "      <td>2024-08-03 19:23:04.347273</td>\n",
       "      <td>47.99</td>\n",
       "      <td>83.14</td>\n",
       "      <td>19.64</td>\n",
       "      <td>7.73</td>\n",
       "      <td>4.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensor_1</td>\n",
       "      <td>2024-08-03 19:33:04.347273</td>\n",
       "      <td>52.12</td>\n",
       "      <td>51.77</td>\n",
       "      <td>17.35</td>\n",
       "      <td>7.87</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensor_1</td>\n",
       "      <td>2024-08-03 19:43:04.347273</td>\n",
       "      <td>51.55</td>\n",
       "      <td>79.70</td>\n",
       "      <td>20.05</td>\n",
       "      <td>6.53</td>\n",
       "      <td>1.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sensor_1</td>\n",
       "      <td>2024-08-03 19:53:04.347273</td>\n",
       "      <td>51.23</td>\n",
       "      <td>56.14</td>\n",
       "      <td>26.00</td>\n",
       "      <td>6.44</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sensor ID                  Timestamp  Flow_Rate  Pressure  Temperature  \\\n",
       "0  Sensor_1 2024-08-03 19:13:04.347273      38.99     52.90        21.92   \n",
       "1  Sensor_1 2024-08-03 19:23:04.347273      47.99     83.14        19.64   \n",
       "2  Sensor_1 2024-08-03 19:33:04.347273      52.12     51.77        17.35   \n",
       "3  Sensor_1 2024-08-03 19:43:04.347273      51.55     79.70        20.05   \n",
       "4  Sensor_1 2024-08-03 19:53:04.347273      51.23     56.14        26.00   \n",
       "\n",
       "     pH  Turbidity  \n",
       "0  7.73       2.08  \n",
       "1  7.73       4.73  \n",
       "2  7.87       7.25  \n",
       "3  6.53       1.39  \n",
       "4  6.44       0.77  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9981f6f-0afb-4e61-a984-ff5107913a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x77fed81c00d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dash\n",
    "from dash.dependencies import Input, Output\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import plotly.express as px\n",
    "\n",
    "# Assuming you have a DataFrame 'df' containing your water monitoring data\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1('Water Monitoring Dashboard'),\n",
    "    \n",
    "    # Dropdown for selecting the sensor ID\n",
    "    html.Div([\n",
    "        html.Label(\"Select Sensor ID\"),\n",
    "        dcc.Dropdown(\n",
    "            id='sensor-dropdown',\n",
    "            options=[{'label': i, 'value': i} for i in df['Sensor ID'].unique()],\n",
    "            value=df['Sensor ID'].unique()[0],\n",
    "            placeholder=\"Select Sensor ID\"\n",
    "        )\n",
    "    ], style={'width': '48%', 'display': 'inline-block'}),\n",
    "    \n",
    "    # Dropdown for selecting variables for the time series graph\n",
    "    html.Div([\n",
    "        html.Label(\"Select Variables for Time Series\"),\n",
    "        dcc.Dropdown(\n",
    "            id='variable-dropdown',\n",
    "            options=[{'label': col, 'value': col} for col in df.columns if col not in ['Sensor ID', 'Timestamp']],\n",
    "            multi=True,\n",
    "            placeholder=\"Select Variables for Time Series\"\n",
    "        ),\n",
    "    ], style={'width': '48%', 'display': 'inline-block'}),\n",
    "\n",
    "    # Time series graph\n",
    "    dcc.Graph(id='time-series-graph'),\n",
    "\n",
    "    # Histogram feature selection and graph\n",
    "    html.Div([\n",
    "        html.Label(\"Select Feature for Histogram\"),\n",
    "        dcc.Dropdown(\n",
    "            id='histogram-feature-dropdown',\n",
    "            options=[{'label': col, 'value': col} for col in df.columns if col not in ['Sensor ID', 'Timestamp']],\n",
    "            value='Flow_Rate',  # Default value for histogram\n",
    "            placeholder=\"Select Feature for Histogram\"\n",
    "        ),\n",
    "        dcc.Graph(id='histogram-graph')\n",
    "    ], style={'width': '48%', 'display': 'inline-block'}),\n",
    "\n",
    "    # Box plot feature selection and graph\n",
    "    html.Div([\n",
    "        html.Label(\"Select Feature for Box Plot\"),\n",
    "        dcc.Dropdown(\n",
    "            id='boxplot-feature-dropdown',\n",
    "            options=[{'label': col, 'value': col} for col in df.columns if col not in ['Sensor ID', 'Timestamp']],\n",
    "            value='Turbidity',  # Default value for box plot\n",
    "            placeholder=\"Select Feature for Box Plot\"\n",
    "        ),\n",
    "        dcc.Graph(id='box-plot-graph')\n",
    "    ], style={'width': '48%', 'display': 'inline-block'})\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    [Output('time-series-graph', 'figure'),\n",
    "     Output('histogram-graph', 'figure'),\n",
    "     Output('box-plot-graph', 'figure')],\n",
    "    [Input('sensor-dropdown', 'value'),\n",
    "     Input('variable-dropdown', 'value'),\n",
    "     Input('histogram-feature-dropdown', 'value'),\n",
    "     Input('boxplot-feature-dropdown', 'value')]\n",
    ")\n",
    "def update_graphs(sensor_id, selected_variables, histogram_feature, boxplot_feature):\n",
    "    filtered_df = df[df['Sensor ID'] == sensor_id]\n",
    "\n",
    "    # Time Series Plot (Limit data points, allow selection)\n",
    "    fig1 = px.line(filtered_df.iloc[:500], x='Timestamp', y=selected_variables)\n",
    "    \n",
    "    # Histogram Plot for selected histogram feature\n",
    "    fig2 = px.histogram(filtered_df, x=histogram_feature)\n",
    "    \n",
    "    # Box Plot for selected box plot feature\n",
    "    fig3 = px.box(filtered_df, x=boxplot_feature)\n",
    "\n",
    "    return fig1, fig2, fig3\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514dd0f-06ad-485c-93f1-0abdfa069e58",
   "metadata": {},
   "source": [
    "<center><h1>Training the Model and Testing for Anamolies</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b029a7a-3584-43e1-8925-136eac601b10",
   "metadata": {},
   "source": [
    "##  Custom Isolation Forest\n",
    "Here's a general outline:\n",
    "\n",
    "**1. Label Most Data as Normal**: Assign a label (e.g., label = 0) to the majority of the data, assuming most of it is normal.<br>\n",
    "**2. Randomly Sample and Label Anomalies**: Randomly select a small subset of the data and label them as 1 (anomalies). The exact proportion can be adjusted based on your expectations for anomaly prevalence.<br>\n",
    "**3. Train a Random Forest Classifier** using these labeled data points to distinguish anomalies based on decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d1bbb56-eb61-4f8c-b3db-190c424f87b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/11 19:18:17 WARN Utils: Your hostname, ishan-HP-Laptop-15s-fq5xxx resolves to a loopback address: 127.0.1.1; using 192.168.1.145 instead (on interface wlo1)\n",
      "24/11/11 19:18:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/11 19:18:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+--------+-----------+---+---------+--------+-------------+-----------+----------+\n",
      "|Sensor ID|Timestamp|Flow_Rate|Pressure|Temperature|pH |Turbidity|features|rawPrediction|probability|prediction|\n",
      "+---------+---------+---------+--------+-----------+---+---------+--------+-------------+-----------+----------+\n",
      "+---------+---------+---------+--------+-----------+---+---------+--------+-------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"AnomalyDetection\").getOrCreate()\n",
    "# Step 1: Assemble features\n",
    "data_spark = spark.createDataFrame(df)\n",
    "\n",
    "def create_isolation_forest_approximation(df: DataFrame, feature_columns, contamination=0.05):\n",
    "    # Define VectorAssembler to assemble features\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    data = assembler.transform(df)\n",
    "    \n",
    "    # Label data: Assuming most data is normal\n",
    "    normal_df = data.withColumn(\"label\", F.lit(0))\n",
    "\n",
    "    # Sample a small fraction as anomalies\n",
    "    anomaly_df = normal_df.sample(fraction=contamination).withColumn(\"label\", F.lit(1))\n",
    "    training_df = normal_df.union(anomaly_df)  # Combine labeled data\n",
    "    \n",
    "    # Train the Random Forest model\n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50)\n",
    "    model = rf.fit(training_df)\n",
    "    \n",
    "    # Predict anomalies in the original data\n",
    "    predictions = model.transform(data)\n",
    "    return predictions\n",
    "\n",
    "feature_columns = [\"Flow_Rate\", \"Turbidity\" ,\"Pressure\" ,\"Temperature\"]\n",
    "\n",
    "predictions = create_isolation_forest_approximation(data_spark,feature_columns)\n",
    "\n",
    "# Filter to show only rows identified as anomalies\n",
    "anomalies = predictions.filter(F.col(\"prediction\") == 1)\n",
    "\n",
    "# Show anomalies\n",
    "anomalies.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d702177-dab5-42d2-80b2-2f17bbf8f1f8",
   "metadata": {},
   "source": [
    "Unable to detect any anamolies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f66bf8-92d1-4c57-8921-8224af3ea91d",
   "metadata": {},
   "source": [
    "## Custom Clustering-Based Isolation Using KMeans\n",
    "\n",
    "Here's a general outline:\n",
    "\n",
    "**1. Cluster the data** into `K` clusters using KMeans.<br>\n",
    "**2. Calculate distance** of each point from the nearest cluster center.<br>\n",
    "**3. Flag outliers** as those points whose distance from the centroid is greater than a specified threshold (e.g., based on standard deviation or percentile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de3cde5e-cf60-4708-8803-31e1d58d715c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/11 19:18:43 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------+--------+-----------+----+---------+-------------+-------+------------------+\n",
      "|Sensor ID|           Timestamp|Flow_Rate|Pressure|Temperature|  pH|Turbidity|     features|cluster|          distance|\n",
      "+---------+--------------------+---------+--------+-----------+----+---------+-------------+-------+------------------+\n",
      "| Sensor_1|2024-08-03 21:33:...|    33.93|   43.02|      18.85|6.97|    15.02|[33.93,15.02]|      1|12.916060578920192|\n",
      "| Sensor_1|2024-08-04 06:13:...|    42.91|   45.96|      17.26|6.77|     8.47| [42.91,8.47]|      0| 6.291805322146821|\n",
      "| Sensor_1|2024-08-04 11:23:...|    72.89|   46.26|      25.75|6.84|      0.6|  [72.89,0.6]|      4|5.8424332484811226|\n",
      "| Sensor_1|2024-08-04 22:33:...|    74.77|   37.93|      11.69|6.59|     3.99| [74.77,3.99]|      4| 7.705392975456011|\n",
      "| Sensor_1|2024-08-05 00:03:...|    25.72|   30.39|      25.68| 7.2|     1.07| [25.72,1.07]|      1| 7.025247990928396|\n",
      "| Sensor_1|2024-08-05 01:53:...|    38.03|   49.42|      17.84| 6.5|     10.5| [38.03,10.5]|      0| 9.369970433315403|\n",
      "| Sensor_1|2024-08-05 03:33:...|    24.84|   42.15|      20.29|6.77|     1.24| [24.84,1.24]|      1| 7.873844786985477|\n",
      "| Sensor_1|2024-08-05 04:23:...|    76.62|   63.14|      14.47|6.09|     2.94| [76.62,2.94]|      4| 9.375211476900837|\n",
      "| Sensor_1|2024-08-05 05:23:...|     26.9|   58.81|       22.1|6.47|      0.9|   [26.9,0.9]|      1| 5.896770985561282|\n",
      "| Sensor_1|2024-08-05 16:53:...|    60.46|   57.58|      19.65| 6.9|    24.82|[60.46,24.82]|      2|22.793652819965036|\n",
      "| Sensor_1|2024-08-05 17:03:...|    49.49|   57.83|      15.57|6.58|    12.28|[49.49,12.28]|      3|10.113535321247104|\n",
      "| Sensor_1|2024-08-05 17:53:...|    82.09|   53.46|      22.32| 6.9|     1.23| [82.09,1.23]|      4| 14.84891320558503|\n",
      "| Sensor_1|2024-08-06 09:53:...|    79.91|   41.84|      19.97|6.19|     0.32| [79.91,0.32]|      4|12.776977890006163|\n",
      "| Sensor_1|2024-08-06 12:23:...|    24.86|   66.98|      18.05|8.28|      1.5|  [24.86,1.5]|      1| 7.827568385813371|\n",
      "| Sensor_1|2024-08-06 18:43:...|    76.65|   41.05|      15.72|6.66|     3.75| [76.65,3.75]|      4| 9.502490834448967|\n",
      "| Sensor_1|2024-08-06 19:53:...|    57.32|   44.62|      14.34|6.78|     9.19| [57.32,9.19]|      2| 6.999438506063589|\n",
      "| Sensor_1|2024-08-06 21:33:...|    56.58|   56.35|      25.43|6.36|    14.94|[56.58,14.94]|      2|12.786898967438438|\n",
      "| Sensor_1|2024-08-06 21:43:...|    46.95|   45.38|      22.94|7.03|     7.23| [46.95,7.23]|      3| 5.890880475417616|\n",
      "| Sensor_1|2024-08-07 03:03:...|    59.25|   69.36|      23.51| 7.0|    24.58|[59.25,24.58]|      2|22.437476071872812|\n",
      "| Sensor_1|2024-08-07 04:13:...|    74.25|   75.32|      20.89|7.17|     1.84| [74.25,1.84]|      4| 6.986362428627877|\n",
      "+---------+--------------------+---------+--------+-----------+----+---------+-------------+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col, sqrt\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "data_spark = spark.createDataFrame(df)\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(inputCols=[\"Flow_Rate\", \"Turbidity\"], outputCol=\"features\")\n",
    "data = assembler.transform(data_spark)\n",
    "\n",
    "# Train KMeans model\n",
    "kmeans = KMeans(k=5, featuresCol=\"features\", predictionCol=\"cluster\")\n",
    "model = kmeans.fit(data)\n",
    "\n",
    "# Get cluster centers and add distances to the DataFrame\n",
    "centers = model.clusterCenters()\n",
    "data = model.transform(data)\n",
    "\n",
    "# Define a UDF to compute Euclidean distance to the nearest center\n",
    "def calculate_distance(features, cluster):\n",
    "    center = centers[cluster]\n",
    "    return float(sum((features[i] - center[i]) ** 2 for i in range(len(center))) ** 0.5)\n",
    "\n",
    "distance_udf = F.udf(calculate_distance, DoubleType())\n",
    "data = data.withColumn(\"distance\", distance_udf(\"features\", \"cluster\"))\n",
    "\n",
    "# Define threshold based on the distribution of distances\n",
    "threshold = data.selectExpr(\"percentile(distance, 0.95)\").collect()[0][0]  # 95th percentile as threshold\n",
    "anomalies = data.filter(col(\"distance\") > threshold)\n",
    "\n",
    "anomalies.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aef579-d3e3-4352-bcf5-0ca2cf07ac1c",
   "metadata": {},
   "source": [
    "## Mahalanobis Distance for Multivariate Outlier Detection\n",
    "\n",
    "Using **Mahalanobis distance** on multiple features can identify anomalies, especially when features are correlated. It measures the distance from the center (mean vector) scaled by the covariance.\n",
    "\n",
    "**1. Compute the mean vector** and **covariance matrix** of the features.<br>\n",
    "**2. Calculate Mahalanobis distance** for each point.<br>\n",
    "3.Set a threshold (such as the Chi-square critical value) to label points as anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ad443c4-e9ec-4f67-8e5b-3f89ef44190d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 81:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------+--------+-----------+----+---------+--------------------+--------------------+\n",
      "|Sensor ID|           Timestamp|Flow_Rate|Pressure|Temperature|  pH|Turbidity|            features|mahalanobis_distance|\n",
      "+---------+--------------------+---------+--------+-----------+----+---------+--------------------+--------------------+\n",
      "| Sensor_1|2024-08-05 16:53:...|    60.46|   57.58|      19.65| 6.9|    24.82|[60.46,24.82,57.5...|  11.913321220470868|\n",
      "| Sensor_1|2024-08-07 03:03:...|    59.25|   69.36|      23.51| 7.0|    24.58|[59.25,24.58,69.3...|  11.833324250278647|\n",
      "| Sensor_1|2024-08-18 15:03:...|    38.81|   58.18|       18.3|7.27|     22.4|[38.81,22.4,58.18...|  10.671856708733037|\n",
      "| Sensor_1|2024-08-26 23:43:...|    52.31|   48.64|      22.64|6.66|    23.44|[52.31,23.44,48.6...|  11.177205866595084|\n",
      "| Sensor_1|2024-08-27 08:13:...|    60.94|   35.59|      12.12|7.41|    17.67|[60.94,17.67,35.5...|   8.465705990339437|\n",
      "| Sensor_1|2024-08-28 00:23:...|    50.31|   72.27|      15.22|7.15|    20.68|[50.31,20.68,72.2...|   9.805459118726361|\n",
      "| Sensor_1|2024-08-28 08:23:...|    44.13|   41.62|      10.07|7.21|    23.24|[44.13,23.24,41.6...|  11.280585444272491|\n",
      "| Sensor_1|2024-08-31 22:13:...|    48.34|   47.95|      26.39| 7.2|    17.08|[48.34,17.08,47.9...|   7.943339617579485|\n",
      "| Sensor_1|2024-09-15 17:33:...|    55.09|   41.43|       21.0|6.93|    25.06|[55.09,25.06,41.4...|  12.055737908352118|\n",
      "| Sensor_1|2024-09-17 09:53:...|    52.44|  167.61|      11.67|6.57|     2.19|[52.44,2.19,167.6...|   7.939221285308664|\n",
      "| Sensor_1|2024-09-25 02:23:...|    41.73|   47.84|      20.93|6.86|    24.16|[41.73,24.16,47.8...|  11.577948460308981|\n",
      "| Sensor_1|2024-09-27 14:23:...|    41.91|   61.25|      22.75|7.15|    17.87|[41.91,17.87,61.2...|    8.29665663115233|\n",
      "| Sensor_1|2024-09-28 20:53:...|     54.4|   52.09|      11.79|7.46|    22.25|[54.4,22.25,52.09...|  10.658862062357514|\n",
      "| Sensor_1|2024-10-11 14:33:...|    57.64|    51.0|      22.96|6.67|    17.36|[57.64,17.36,51.0...|   8.023427885418757|\n",
      "| Sensor_1|2024-10-11 19:03:...|    43.27|   56.04|      21.67|7.63|    19.75|[43.27,19.75,56.0...|   9.247504185966744|\n",
      "| Sensor_1|2024-10-14 14:23:...|    59.36|   47.98|      20.09|6.94|    23.54|[59.36,23.54,47.9...|  11.251681192967562|\n",
      "| Sensor_1|2024-10-21 15:33:...|    64.96|   41.17|      12.65|7.16|     19.6|[64.96,19.6,41.17...|    9.42482738215988|\n",
      "| Sensor_1|2024-10-22 18:43:...|    31.73|   46.92|      23.65|6.84|    16.61|[31.73,16.61,46.9...|  7.8560574779718895|\n",
      "| Sensor_1|2024-10-31 09:33:...|    64.12|   62.97|      19.59|7.29|    16.96|[64.12,16.96,62.9...|   7.881570619572849|\n",
      "| Sensor_1|2024-11-05 18:13:...|    53.88|   37.66|      16.89|6.65|    22.47|[53.88,22.47,37.6...|  10.743604606017076|\n",
      "+---------+--------------------+---------+--------+-----------+----+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"AnomalyDetection\").getOrCreate()\n",
    "# Step 1: Assemble features\n",
    "data_spark = spark.createDataFrame(df)\n",
    "assembler = VectorAssembler(inputCols=[\"Flow_Rate\", \"Turbidity\" ,\"Pressure\" ,\"Temperature\"], outputCol=\"features\")\n",
    "data = assembler.transform(data_spark)\n",
    "\n",
    "# Step 2: Calculate mean and covariance\n",
    "mean_vector = np.array(data.select(\"features\").rdd.map(lambda x: x[0]).mean())\n",
    "cov_matrix = np.array(data.select(\"features\").rdd.map(lambda x: np.outer(x[0] - mean_vector, x[0] - mean_vector)).mean())\n",
    "\n",
    "# Step 3: Define Mahalanobis distance function\n",
    "def mahalanobis_distance(row, mean, cov_inv):\n",
    "    x = np.array(row)\n",
    "    return float(np.sqrt((x - mean).T @ cov_inv @ (x - mean)))\n",
    "\n",
    "# Invert covariance matrix\n",
    "cov_matrix_inv = np.linalg.inv(cov_matrix)\n",
    "mahalanobis_udf = F.udf(lambda x: mahalanobis_distance(x, mean_vector, cov_matrix_inv))\n",
    "\n",
    "# Step 4: Add Mahalanobis distance column and filter anomalies\n",
    "data = data.withColumn(\"mahalanobis_distance\", mahalanobis_udf(\"features\"))\n",
    "\n",
    "# Set threshold for anomalies (e.g., Chi-square critical value for 95% confidence)\n",
    "threshold = 7.815  # For 2 degrees of freedom at 95% confidence\n",
    "anomalies = data.filter(col(\"mahalanobis_distance\") > threshold)\n",
    "anomalies.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930dbbc7-580c-4d7d-941a-ccc85d1f780b",
   "metadata": {},
   "source": [
    "\n",
    "# Importing Required Modules\n",
    "\n",
    "In this cell, we import necessary modules from `kafka-python`. Specifically, we import:\n",
    "- `KafkaProducer`: for sending messages to a Kafka topic.\n",
    "- `KafkaConsumer`: for reading messages from a Kafka topic.\n",
    "  \n",
    "These modules provide the core functionality needed for Kafka interactions.\n",
    "\n",
    "\n",
    "# Setting Up the Kafka Producer\n",
    "\n",
    "Here, we create a Kafka Producer, which will allow us to send messages to a Kafka topic. We configure the producer with the following parameters:\n",
    "- `bootstrap_servers`: The address of the Kafka server, typically set as \"localhost:9092\" for local testing.\n",
    "- `value_serializer`: Serializes message data to JSON format, making it easier to handle structured data in Kafka.\n",
    "\n",
    "The producer can now send JSON-encoded messages to Kafka.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5f93c2e-3622-4578-a19b-d27575325601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80fd4213b8d46aa859cf2abaa4b7673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Flow Rate</h3>'), VBox(children=(FloatSlider(value=150.0, description='Flow Rat…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "# Initialize Kafka Producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda x: json.dumps(x).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Define the interactive widgets for mean and sigma\n",
    "flow_rate_mean_slider = widgets.FloatSlider(value=150, min=50, max=200, step=1, description='Flow Rate Mean:')\n",
    "flow_rate_sigma_slider = widgets.FloatSlider(value=10, min=1, max=20, step=1, description='Flow Rate Sigma:')\n",
    "\n",
    "turbidity_mean_slider = widgets.FloatSlider(value=0.5, min=0, max=5, step=0.1, description='Turbidity Mean:')\n",
    "turbidity_sigma_slider = widgets.FloatSlider(value=0.75, min=0.1, max=2, step=0.1, description='Turbidity Sigma:')\n",
    "\n",
    "temperature_mean_slider = widgets.FloatSlider(value=20, min=10, max=30, step=0.5, description='Temperature Mean:')\n",
    "temperature_sigma_slider = widgets.FloatSlider(value=5, min=1, max=10, step=1, description='Temperature Sigma:')\n",
    "\n",
    "pressure_mean_slider = widgets.FloatSlider(value=4, min=2, max=6, step=0.1, description='Pressure Mean:')\n",
    "pressure_sigma_slider = widgets.FloatSlider(value=0.25, min=0.1, max=1, step=0.05, description='Pressure Sigma:')\n",
    "\n",
    "# Slider for the number of messages\n",
    "num_messages_slider = widgets.IntSlider(value=50, min=10, max=1000, step=10, description='Number of Messages:')\n",
    "\n",
    "send_button = widgets.Button(description=\"Send Data\")\n",
    "\n",
    "# Create layouts for each sensor parameter\n",
    "flow_rate_box = widgets.VBox([flow_rate_mean_slider, flow_rate_sigma_slider])\n",
    "turbidity_box = widgets.VBox([turbidity_mean_slider, turbidity_sigma_slider])\n",
    "temperature_box = widgets.VBox([temperature_mean_slider, temperature_sigma_slider])\n",
    "pressure_box = widgets.VBox([pressure_mean_slider, pressure_sigma_slider])\n",
    "\n",
    "# Organize all widgets into a main layout\n",
    "main_box = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Flow Rate</h3>\"),\n",
    "    flow_rate_box,\n",
    "    widgets.HTML(\"<h3>Turbidity</h3>\"),\n",
    "    turbidity_box,\n",
    "    widgets.HTML(\"<h3>Temperature</h3>\"),\n",
    "    temperature_box,\n",
    "    widgets.HTML(\"<h3>Pressure</h3>\"),\n",
    "    pressure_box,\n",
    "    num_messages_slider,\n",
    "\n",
    "    send_button\n",
    "])\n",
    "\n",
    "# Display the main layout\n",
    "display(main_box)\n",
    "\n",
    "# Define the function to send data using the slider values\n",
    "def send_data(button):\n",
    "    clear_output(wait=True)  # Clear previous outputs\n",
    "    display(main_box)  # Redisplay the main box after clearing\n",
    "    num_messages = num_messages_slider.value\n",
    "    for _ in range(num_messages):\n",
    "        data = {\n",
    "            'sensor_id': 'sensor_1',\n",
    "            'timestamp': int(time.time()),\n",
    "            'flow_rate': round(random.gauss(mu=flow_rate_mean_slider.value, sigma=flow_rate_sigma_slider.value), 2),\n",
    "            'turbidity': round(random.lognormvariate(mu=turbidity_mean_slider.value, sigma=turbidity_sigma_slider.value), 2),\n",
    "            'temperature': round(random.gauss(mu=temperature_mean_slider.value, sigma=temperature_sigma_slider.value), 2),\n",
    "            'pressure': round(random.lognormvariate(mu=pressure_mean_slider.value, sigma=pressure_sigma_slider.value), 2),\n",
    "        }\n",
    "        producer.send('test-topic', value=data)\n",
    "        print(f\"Sent data: {data}\")\n",
    "        time.sleep(0.1)  # Optional: add a small delay between messages\n",
    "\n",
    "# Link the button to the function\n",
    "send_button.on_click(send_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75410151-5c9e-4bbb-bcaa-eb9bbea07e1a",
   "metadata": {},
   "source": [
    "## Sending the Trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "422ddd0e-65b3-4aa2-a6f6-f1a887eef052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    ")\n",
    "\n",
    "def send_model():\n",
    "    # Serialize the array\n",
    "    serialized_array = pickle.dumps(np.array(mean_vector))\n",
    "    \n",
    "    # Send the serialized array\n",
    "    producer.send('test-topic', value=serialized_array)\n",
    "    producer.flush()\n",
    "\n",
    "    # Serialize the array\n",
    "    serialized_array = pickle.dumps(np.array(cov_matrix))\n",
    "    \n",
    "    # Send the serialized array\n",
    "    producer.send('test-topic', value=serialized_array)\n",
    "    producer.flush()\n",
    "    \n",
    "send_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d512078-e288-4523-be1e-511bd284148d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
